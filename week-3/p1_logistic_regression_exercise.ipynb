{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Classification - Logistic Regression (Exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is similar to regression except that the output variable is a discrete category as opposed to a continuous value. The goal of classiﬁcation is to accurately predict the target class for each case in the data.\n",
    "\n",
    "The most common form of classiﬁcation is binary classiﬁcation where examples are assigned one of two labels. However, there is also multi-class classiﬁcation where examples are assigned one of more than two labels. A common example of multi-class classiﬁcation is handwritten digit recognition where the aim is to assign each input vector to one of a ﬁnite number of discrete categories.\n",
    "\n",
    "This notebook will be focused on **logistic regression** which is a binary classiﬁcation algorithm. It is a simple and powerful linear classiﬁcation algorithm. It also serves as a good example of applying a supervised learning algorithm to a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seaborn import heatmap\n",
    "\n",
    "def plot_loss_curve(logs):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    plt.plot(logs)\n",
    "    plt.show()\n",
    "\n",
    "def return_binary(y_train, y_test, classNum):\n",
    "    trainIndices = y_train == classNum\n",
    "    testIndices = y_test == classNum\n",
    "    y_train_binary = np.zeros(y_train.shape)\n",
    "    y_test_binary = np.zeros(y_test.shape)\n",
    "    y_train_binary[trainIndices] = 1\n",
    "    y_test_binary[testIndices] = 1\n",
    "    return y_train_binary, y_test_binary\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "\n",
    "Below is the code for loading sklearn's breast cancer dataset and splitting it into train and test sets. For simplicity, we will only look at two classes for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "CLASSNUM = 0\n",
    "y_train_binary, y_test_binary = return_binary(y_train, y_test, CLASSNUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression implementation using Scikit-Learn\n",
    "\n",
    "Below, write the code that uses Scikit-Learn's logsitic regression model, trains it and uses it to get our y binary predictions for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "### Implement logistic regression model, train it and get predictions.\n",
    "\n",
    "logistic_regression = None\n",
    "y_pred = None\n",
    "\n",
    "### \n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test_binary, y_pred))\n",
    "print(\"Precision: \", precision_score(y_test_binary, y_pred))\n",
    "_ = ConfusionMatrixDisplay.from_predictions(y_test_binary, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside - The Sigmoid Function\n",
    "\n",
    "The sigmoid function is an activation function that you will see extremely often in neural networks. It is defined as follows:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "What it essentially does is smoothly squish any value to a value between 0 and 1. This is useful especially for binary classification where we want to classify an example as either 0 or 1.\n",
    "\n",
    "You will see this function a lot in subsequent notebooks. Please implement the sigmoid function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ### SOLUTION\n",
    "    sig = np.zeros(z.shape)\n",
    "    ### SOLUTION\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression using Gradient Descent\n",
    "\n",
    "You saw gradient descent last week in terms of linear regression. We can use it for logistic regression too, however this time we will use a different loss function.\n",
    "\n",
    "In this notebook, we will use the logistic loss function. It is defined as follows:\n",
    "\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(\\hat{y}) + (1 - y^{(i)})\\log(1 - \\hat{y})$$\n",
    "\n",
    "where $\\hat{y} = h_{\\theta}(x^{(i)}) = \\sigma(\\theta^{T}x^{(i)})$.\n",
    "\n",
    "Don't worry if this looks too complicated! In a nutshell, like other loss functions, it is a measure of how well our model is doing. The lower the loss, the better our model is doing. A simplified version of the loss function is as follows:\n",
    "\n",
    "Since $y^{(i)}$ can only be 0 or 1, each individual loss calculation can be simplified to\n",
    "\n",
    "$$\\mathcal{L}(\\hat{y}, y) = \\begin{cases} -\\log(\\hat{y}) & \\text{if } y = 1 \\\\ -\\log(1 - \\hat{y}) & \\text{if } y = 0 \\end{cases}$$\n",
    "\n",
    "The total loss is then the average of all the individual losses.\n",
    "\n",
    "Finding the gradient of this loss function is a little more complicated than linear regression, but it is still doable. The gradient is as follows:\n",
    "\n",
    "$$\\nabla_{\\theta}\\mathcal{L}(\\theta) = \\frac{1}{m}x^{T}(\\hat{y} - y)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add the tiny numbers in the logs because log(0) is undefined\n",
    "def loss(preds, targets):\n",
    "    preds = preds.flatten()\n",
    "    loss_calc = -np.mean(targets * np.log(preds + 1e-200) + (1 - targets) * np.log(1 - preds + 1e-200))\n",
    "    return loss_calc\n",
    "\n",
    "def dloss(preds, targets, X):\n",
    "    targets = targets.reshape(-1, 1)\n",
    "    return X.T @ (preds - targets) / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fill in our hypothesis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(X, W):\n",
    "    ### Implement the hypothesis function\n",
    "    hyp = np.zeros(X.shape[0], 1)\n",
    "    ### \n",
    "    return hyp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have completed enough components to implement gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, W, learning_rate, num_iterations):\n",
    "\n",
    "    history = []\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        ### Put everything together by obtaining predicitons with hypothesis,\n",
    "        ### computing the cost, appending cost to history and computing the gradient\n",
    "        ### as well as updating the weights.\n",
    "\n",
    "        preds = 0\n",
    "        cost = 0\n",
    "        history.append(cost)\n",
    "        gradient = 0\n",
    "        W -= 0\n",
    "        \n",
    "        ### \n",
    "    \n",
    "    return W, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets put together our Classification pipeline by firstly setting our X train and test sets, initialize the weights, train our model with gradient descent, then evaluate the model performance by looking at the loss curve and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data binary\n",
    "\n",
    "# Add bias\n",
    "X_train_biased = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "X_test_biased = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "\n",
    "# Starting weights\n",
    "W = np.zeros((X_train_biased.shape[1], 1))\n",
    "\n",
    "# Train\n",
    "W, history = gradient_descent(X_train_biased, y_train_binary, W, 0.001, 400)\n",
    "\n",
    "# Plot loss\n",
    "plot_loss_curve(history)\n",
    "\n",
    "# Test\n",
    "y_pred = hypothesis(X_test_biased, W)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test_binary, y_pred))\n",
    "print(\"Precision: \", precision_score(y_test_binary, y_pred))\n",
    "_ = ConfusionMatrixDisplay.from_predictions(y_test_binary, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
