{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Classification Part 3 - Support Vector Machines (SVM) and Decision Trees (Exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a brief introduction to **Support Vector Machines** (SVMs) and **Decision Trees**.\n",
    "\n",
    "For both methods, we will show two examples. One after using PCA to reduce the dimensionality of the data, and one with just using two features from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "labels = data.feature_names\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "### Implement a PCA that reduces the dataset to 2 dimensions.\n",
    "pca = None\n",
    "pca_2d = None\n",
    "###\n",
    "\n",
    "# Two features\n",
    "X_two_features = X[:, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVMs using Scikit-Learn\n",
    "\n",
    "Our code in this section shows how Support Vector Machines (SVMs) can be used with Scikit-Learn. SVM helps us find the optimal hyperplane to separate different classes. We will demonstrate how to create, train, and evaluate an SVM model, showcasing its versatility and ability to handle complex decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train SVM\n",
    "\n",
    "### Implement and train your svm using pca_2d, use random state 42.\n",
    "svm = None\n",
    "###\n",
    "\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    svm,\n",
    "    pca_2d,\n",
    "    response_method=\"predict\",\n",
    "    ax=ax,\n",
    "    alpha=0.8,\n",
    "    cmap=plt.cm.coolwarm\n",
    ")\n",
    "ax.scatter(pca_2d[:, 0], pca_2d[:, 1], c=y, s=75, cmap=plt.cm.coolwarm, edgecolor=\"k\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train SVM\n",
    "\n",
    "### Implement and train your svm using X_two_features, use random state 42.\n",
    "svm2f = None\n",
    "###\n",
    "\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    svm2f,\n",
    "    X_two_features,\n",
    "    response_method=\"predict\",\n",
    "    ax=ax,\n",
    "    alpha=0.8,\n",
    "    cmap=plt.cm.coolwarm\n",
    ")\n",
    "ax.scatter(X_two_features[:, 0], X_two_features[:, 1], c=y, s=75, cmap=plt.cm.coolwarm, edgecolor=\"k\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees using Scikit-Learn\n",
    "\n",
    "With this code demonstration, we will illustrate how Decision Trees can be implemented using Scikit-Learn. Scikit-Learn's decision tree classifier will be used to build and visualize a simple decision tree model, illustrating its effectiveness for making decisions.\n",
    "\n",
    "The DecisionTreeClassifier recursively partitions the dataset into subsets based on the values of its input features. By choosing the most appropriate feature to split on at each step, it aims to minimize impurities within subsets. This process continues until a stopping criterion is reached, such as a maximum depth or nodes with pure, unambiguous classes. By following the branches of the tree based on the input features, the resulting tree structure can be used for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree in sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# Train Decision Tree\n",
    "\n",
    "### Implement and train your svm using pca_2d, use random state 42.\n",
    "dt = None\n",
    "###\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    dt,\n",
    "    pca_2d,\n",
    "    response_method=\"predict\",\n",
    "    ax=ax,\n",
    "    alpha=0.8,\n",
    "    cmap=plt.cm.coolwarm,\n",
    "    xlabel=\"PCA1\",\n",
    "    ylabel=\"PCA2\"\n",
    ")\n",
    "ax.scatter(pca_2d[:, 0], pca_2d[:, 1], c=y, s=75, cmap=plt.cm.coolwarm, edgecolor=\"k\")\n",
    "plt.show()\n",
    "\n",
    "# Plot tree\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plot_tree(dt, ax=ax, filled=True, rounded=True, fontsize=10, feature_names=[\"PCA1\", \"PCA2\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree in sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# Train Decision Tree\n",
    "\n",
    "### Implement and train your svm using X_two_features, use random state 42.\n",
    "dt2f = None\n",
    "###\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    dt2f,\n",
    "    X_two_features,\n",
    "    response_method=\"predict\",\n",
    "    ax=ax,\n",
    "    alpha=0.8,\n",
    "    cmap=plt.cm.coolwarm,\n",
    "    xlabel=labels[0],\n",
    "    ylabel=labels[1]\n",
    ")\n",
    "ax.scatter(X_two_features[:, 0], X_two_features[:, 1], c=y, s=75, cmap=plt.cm.coolwarm, edgecolor=\"k\")\n",
    "plt.show()\n",
    "\n",
    "# Plot tree\n",
    "# NOTE - this will produce a big image with very small font. You can zoom in to see the tree.\n",
    "fig, ax = plt.subplots(figsize=(50, 50))\n",
    "plot_tree(dt2f, ax=ax, filled=True, rounded=True, fontsize=10, feature_names=[labels[0], labels[1]])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
